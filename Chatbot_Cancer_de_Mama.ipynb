{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7jJspOeEEaHplmy8zmxRZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuiilhermeOliveira/Projeto_IA_Gemini/blob/main/Chatbot_Cancer_de_Mama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tULR2rdDnYJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a937db3-7b8a-4fe0-859b-9232106e1b92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.9/163.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.3/718.3 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Instala a biblioteca Google Generative AI\n",
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "import textwrap\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Configura a chave de API do Google Generative AI\n",
        "GOOGLE_API_KEY=\"API do Google\"\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "# Define o modelo de embedding a ser usado\n",
        "model = \"models/embedding-001\""
      ],
      "metadata": {
        "id": "bZhUu8mVEimG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para gerar embeddings a partir do título e do texto\n",
        "def embed_fn(title, text):\n",
        "  return genai.embed_content(model=model,\n",
        "                            content=text,\n",
        "                            title=title,\n",
        "                            task_type=\"RETRIEVAL_DOCUMENT\")[\"embedding\"]"
      ],
      "metadata": {
        "id": "7NEdXNv1EkgX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega a planilha Excel contendo os dados\n",
        "file_path = '/content/CancerDeMama.xlsx'  # substitua pelo caminho do seu arquivo\n",
        "df = pd.read_excel(file_path)\n",
        "df = pd.DataFrame(df)\n",
        "# Adiciona uma nova coluna \"Embedding\" ao DataFrame com os embeddings gerados\n",
        "df[\"Embedding\"] = df.apply(lambda row: embed_fn(row[\"Titulo\"], row[\"Conteudo\"]), axis=1)"
      ],
      "metadata": {
        "id": "qSPuTfzAEnoU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para gerar uma consulta e buscar a resposta mais relevante na base de dados\n",
        "def gerar_e_buscar_consulta(consulta, base, model):\n",
        "  embedding_da_consulta = genai.embed_content(model=model,\n",
        "                            content=consulta,\n",
        "                            task_type=\"RETRIEVAL_QUERY\")\n",
        "  produtos_escalares = np.dot(np.stack(df[\"Embedding\"]), embedding_da_consulta[\"embedding\"])\n",
        "\n",
        "  indices = np.argmax(produtos_escalares)\n",
        "  return df.iloc[indices][\"Conteudo\"]"
      ],
      "metadata": {
        "id": "nNFVaNZzEs2L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Configurações de geração de conteúdo\n",
        "generation_config = {\n",
        "    \"candidate_count\": 1,\n",
        "    \"temperature\": 0.5,\n",
        "    \"top_k\": 35,\n",
        "    \"top_p\": 0.95,\n",
        "}\n",
        "# Configurações de segurança para o modelo generativo\n",
        "safety_setting = {\n",
        "    \"HARASSMENT\": \"BLOCK_NONE\",\n",
        "    \"HATE\": \"BLOCK_NONE\",\n",
        "    \"SEXUAL\": \"BLOCK_NONE\",\n",
        "    \"DANGEROUS\": \"BLOCK_NONE\",\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "N9RgnIUOEvDw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define o modelo generativo a ser usado\n",
        "model_2 = genai.GenerativeModel(model_name=\"gemini-1.5-pro\",\n",
        "                                generation_config=generation_config,\n",
        "                                safety_settings=safety_setting)\n",
        "\n",
        "# Função para converter texto em Markdown\n",
        "def to_markdown(text):\n",
        "    text = text.replace('•', '  *')\n",
        "    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "# Função para processar a consulta\n",
        "def processar_consulta(widget):\n",
        "    consulta = widget.value.strip()\n",
        "    if not consulta:\n",
        "        return\n",
        "\n",
        "    if consulta.lower() == \"sair\":\n",
        "        print(\"Encerrando a conversa!!!.\")\n",
        "        widget.close()\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Gerar consulta e obter resposta\n",
        "        trecho = gerar_e_buscar_consulta(consulta, df, model)\n",
        "        prompt = f\"Escreva o texto de forma informativa, sem adicionar informações que não façam parte do texto: {trecho}\"\n",
        "        response = model_2.generate_content(prompt)\n",
        "\n",
        "        # Exibir resposta formatada em Markdown\n",
        "        display(to_markdown(f\"**Resposta**: {response.text}\"))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao processar a consulta: {e}\")\n",
        "\n",
        "    # Remover o widget de input atual\n",
        "    widget.close()\n",
        "\n",
        "    # Criar um novo input abaixo da resposta\n",
        "    new_input = widgets.Text(\n",
        "        value='',\n",
        "        placeholder='Digite sua próxima pergunta aqui (ou digite \"sair\" para encerrar a conversa):',\n",
        "        description='Usuário:',\n",
        "        disabled=False,\n",
        "        layout={'width': '60%'}\n",
        "    )\n",
        "    new_input.on_submit(processar_consulta)\n",
        "    display(new_input)\n",
        "\n",
        "# Widget de input inicial\n",
        "input_widget = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Digite sua pergunta aqui (ou digite \"sair\" para encerrar a conversa):',\n",
        "    description='Usuário:',\n",
        "    disabled=False,\n",
        "    layout={'width': '60%'}\n",
        ")\n",
        "\n",
        "# Conecta a função ao widget com on_submit\n",
        "input_widget.on_submit(processar_consulta)\n",
        "\n",
        "# Exibe o widget inicial\n",
        "display(input_widget)\n"
      ],
      "metadata": {
        "id": "CY996H7SEzxd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}